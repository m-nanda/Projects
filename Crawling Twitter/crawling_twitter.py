# -*- coding: utf-8 -*-
"""Crawling Twitter.ipynb

Automatically generated by Colaboratory.

"""

import tweepy
from datetime import datetime, timedelta
import time 
import pandas as pd

# API key - API Secret
api_key = '...'
api_key_secret = '...'

# Access Token - Token Secret
access_token = '...'
access_token_secret = '...'

# authentication
auth = tweepy.OAuthHandler(api_key, api_key_secret)
auth.set_access_token(access_token, access_token_secret)
api = tweepy.API(auth, wait_on_rate_limit=True)

# settings
tweetsPerQry = 100
query = 'Machine Learning OR Deep Learning'
maxId = -1
tweetCount = 0

# list untuk hasil crawling 
user_name = []
tweet_txt = []

# untuk batasan waktu
interval = 60
t_0 = time.time()
t_n = 0

# looping dalam waktu 1 menit
while t_n <= interval:
  if maxId <= 0:
    newTweets = api.search(q=query, count=tweetsPerQry, result_type='recent', tweet_mode='extended')
  
  else:
    newTweets = api.search(q=query, count=tweetsPerQry, max_id=str(maxId - 1), result_type='recent', tweet_mode='extended')

  if not newTweets:
    print('Tweet Habis')
    break
	
  for tweet in newTweets:
    print(tweet.full_text.encode('utf-8'))
    user_name.append(tweet.user.name)
    tweet_txt.append(tweet.full_text.encode('utf-8'))    
	
  maxId = newTweets[-1].id
  t_n = time.time() - t_0

df = pd.DataFrame({'User Name': user_name, 'Tweet':tweet_txt})
df.head()

df.shape

df.isnull().sum()

df.to_csv('hasil_crawling.txt', header=None, index=None, sep='\t', mode='a')

cek_file_txt = pd.read_csv('hasil_crawling.txt', header=None, sep='\t')
cek_file_txt.head()